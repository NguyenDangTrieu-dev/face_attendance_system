{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# Load file .npy mà không dùng `.item()`\nembeddings_data = np.load(\"/kaggle/input/face-embeddings1/face_embeddings.npy\", allow_pickle=True)\n\nprint(type(embeddings_data))  # Kiểm tra kiểu dữ liệu\nprint(embeddings_data.shape)  # Kiểm tra kích thước\nprint(embeddings_data[:5])    # In 5 dòng đầu tiên","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import defaultdict\n\n# Load file .npy\ndata = np.load(\"/kaggle/input/face-embeddings1/face_embeddings.npy\", allow_pickle=True)\n\n# Dictionary để lưu embeddings\nembeddings_dict = defaultdict(list)\n\n# Lặp qua từng dòng trong mảng\nfor row in data:\n    file_path = row[0]  # Đường dẫn ảnh (bỏ qua nếu không cần)\n    person_id = row[1]  # Tên người\n    embedding = row[2:].astype(np.float32)  # Vector 512 chiều\n\n    # Lưu vào dictionary theo từng người\n    embeddings_dict[person_id].append(embedding)\n\n# Chuyển defaultdict về dictionary bình thường\nembeddings_dict = dict(embeddings_dict)\n\n# Kiểm tra dữ liệu sau khi chuyển đổi\nprint(f\"🔹 Số người trong dataset: {len(embeddings_dict)}\")\nprint(f\"🔹 Số ảnh của người đầu tiên: {len(list(embeddings_dict.values())[0])}\")\nprint(f\"🔹 Một vector đặc trưng:\\n{list(embeddings_dict.values())[0][0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\npairs = []   # Danh sách cặp ảnh\nlabels = []  # Danh sách nhãn (1 hoặc 0)\n\npeople = list(embeddings_dict.keys())\n\nfor person in people:\n    embeddings = embeddings_dict[person]\n    num_samples = len(embeddings)\n\n    for i in range(num_samples - 1):\n        # 📌 Positive pair: Cùng một người, nhãn = 1\n        pairs.append([embeddings[i], embeddings[i + 1]])\n        labels.append(1)\n\n        # 📌 Negative pair: Khác người, nhãn = 0\n        other_person = random.choice([p for p in people if p != person])  # Chọn ngẫu nhiên người khác\n        negative_sample = random.choice(embeddings_dict[other_person])  # Chọn 1 ảnh của người khác\n\n        pairs.append([embeddings[i], negative_sample])\n        labels.append(0)\n\n# Chuyển thành numpy array\npairs = np.array(pairs)\nlabels = np.array(labels)\n\n# 📌 Lưu dataset với các cặp ảnh và nhãn dưới dạng nén\nnp.savez_compressed(\"siamese_dataset.npz\", pairs=pairs, labels=labels)\n\nprint(\"✅ Dataset đã được lưu thành công!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# 📌 Load dataset\ndata = np.load(\"siamese_dataset.npz\")\n\npairs = data[\"pairs\"]  # (N, 2, 512) - Mỗi cặp có 2 vector 512-d\nlabels = data[\"labels\"]  # (N,) - Nhãn 1 hoặc 0\n\nprint(f\"🔹 Số lượng cặp ảnh: {pairs.shape[0]}\")\nprint(f\"🔹 Kích thước vector ảnh: {pairs.shape[1:]}\")  # (2, 512)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Giả sử đã có pairs và labels\nprint(\"📊 Shape dữ liệu ban đầu:\")\nprint(\"pairs shape:\", pairs.shape)\nprint(\"labels shape:\", labels.shape)\n\n# Chia tập train/validation (80-20)\nX_train, X_val, y_train, y_val = train_test_split(\n    pairs,\n    labels, \n    test_size=0.2,\n    random_state=42,\n    stratify=labels\n)\n\nprint(\"\\n✅ Kết quả chia dữ liệu:\")\nprint(f\"Train data: {X_train.shape} (samples), {y_train.shape} (labels)\")\nprint(f\"Val data: {X_val.shape} (samples), {y_val.shape} (labels)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cosine_similarity(tensors):\n    x, y = tensors\n    x_norm = tf.norm(x, axis=-1, keepdims=True)\n    y_norm = tf.norm(y, axis=-1, keepdims=True)\n    return tf.reduce_sum(x * y, axis=-1, keepdims=True) / (x_norm * y_norm + 1e-7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_siamese_model(input_shape=(512,)):\n    # Shared Embedding Model\n    input_layer = Input(shape=input_shape)\n    x = Dense(256, activation='relu')(input_layer)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    embedding = Dense(32, activation='relu')(x)\n    embedding_model = Model(input_layer, embedding)\n    \n    # Siamese Network\n    input_A = Input(shape=input_shape)\n    input_B = Input(shape=input_shape)\n    \n    processed_A = embedding_model(input_A)\n    processed_B = embedding_model(input_B)\n    \n    similarity = Lambda(\n        cosine_similarity,\n        output_shape=(1,),\n        name='cosine_similarity'\n    )([processed_A, processed_B])\n    \n    output = Dense(1, activation='sigmoid')(similarity)\n    return Model(inputs=[input_A, input_B], outputs=output)\n\nsiamese_model = build_siamese_model()\nsiamese_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"siamese_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(\n        monitor='val_loss',\n        patience=7,\n        restore_best_weights=True\n    ),\n    ModelCheckpoint(\n        'best_siamese_model.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max'\n    )\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = siamese_model.fit(\n    [X_train[:, 0], X_train[:, 1]],\n    y_train,\n    validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n    epochs=50,\n    batch_size=64,\n    callbacks=callbacks,\n    verbose=2\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_acc = siamese_model.evaluate([X_val[:, 0], X_val[:, 1]], y_val, verbose=0)\nprint(f\"📊 Validation Accuracy: {val_acc:.4f}\")\nprint(f\"📊 Validation Loss: {val_loss:.4f}\")\n\ny_pred = (siamese_model.predict([X_val[:, 0], X_val[:, 1]]) > 0.5).astype(int)\nprint(\"\\n📝 Classification Report:\")\nprint(classification_report(y_val, y_pred, target_names=['Không khớp', 'Khớp']))\n\n# Confusion Matrix\nplt.figure(figsize=(6, 6))\nsns.heatmap(confusion_matrix(y_val, y_pred), \n            annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Không khớp', 'Khớp'],\n            yticklabels=['Không khớp', 'Khớp'])\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss Curve')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('training_curves.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"🔮 Dự đoán trên 5 mẫu ngẫu nhiên:\")\nsample_idx = np.random.choice(len(X_val), 5)\nfor i, idx in enumerate(sample_idx):\n    prob = siamese_model.predict([X_val[[idx], 0], X_val[[idx], 1]])[0][0]\n    pred = \"Khớp\" if prob > 0.5 else \"Không khớp\"\n    actual = \"Khớp\" if y_val[idx] == 1 else \"Không khớp\"\n    print(f\"\\nMẫu {i+1}:\")\n    print(f\"- Thực tế: {actual}\")\n    print(f\"- Dự đoán: {pred} (Xác suất: {prob:.4f})\")\n    print(\"-\" * 40)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n# 📌 Định nghĩa lại hàm cosine_similarity đúng cách\ndef cosine_similarity(tensors):\n    \"\"\"Hàm tính cosine similarity giữa 2 tensor\"\"\"\n    x, y = tensors  # Unpack 2 tensors đầu vào\n    x_norm = tf.norm(x, axis=-1, keepdims=True)\n    y_norm = tf.norm(y, axis=-1, keepdims=True)\n    similarity = tf.reduce_sum(x * y, axis=-1, keepdims=True) / (x_norm * y_norm + 1e-7)\n    return similarity\n\n# 📌 Load lại model với custom_objects chính xác\nsiamese_model = load_model(\n    'best_siamese_model.keras',\n    custom_objects={'cosine_similarity': cosine_similarity}\n)\n\n# 📌 Kiểm tra model\nprint(\"Input shape:\", siamese_model.input_shape)\nprint(\"Output shape:\", siamese_model.output_shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\n\ndef load_pb_model(model_path):\n    with tf.io.gfile.GFile(model_path, \"rb\") as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(f.read())\n    \n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name=\"\")\n    \n    return graph\n\n# 1. Định nghĩa hàm cosine similarity chính xác\ndef cosine_similarity(tensors):\n    x, y = tensors\n    x_norm = tf.norm(x, axis=-1, keepdims=True)\n    y_norm = tf.norm(y, axis=-1, keepdims=True)\n    return tf.reduce_sum(x * y, axis=-1, keepdims=True) / (x_norm * y_norm + 1e-7)\n\n# 2. Load model với custom objects\nsiamese_model = tf.keras.models.load_model(\n    'best_siamese_model.keras',\n    custom_objects={'cosine_similarity': cosine_similarity}\n)\n\n# 3. Hàm xử lý ảnh và trích xuất embedding (512D từ FaceNet)\ndef get_face_embedding(image_path, sess):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = (img - 127.5) / 128.0  # Chuẩn hóa FaceNet\n    img = np.expand_dims(img, axis=0)\n    \n    embedding = sess.run(\n        embeddings_tensor,\n        feed_dict={input_tensor: img, phase_train_tensor: False}\n    )\n    return embedding.flatten()  # shape (512,)\n\n# 4. Hàm so sánh khuôn mặt hoàn chỉnh\ndef compare_faces(img1_path, img2_path, threshold=0.5):\n    try:\n        # Trích xuất và chuẩn bị dữ liệu\n        emb1 = get_face_embedding(img1_path, facenet_sess).reshape(1, 512)\n        emb2 = get_face_embedding(img2_path, facenet_sess).reshape(1, 512)\n        \n        # Chuẩn hóa L2\n        emb1 = emb1 / np.linalg.norm(emb1)\n        emb2 = emb2 / np.linalg.norm(emb2)\n        \n        # Dự đoán\n        similarity = siamese_model.predict([emb1, emb2], verbose=0)[0][0]\n        return \"MATCH\" if similarity > threshold else \"NO MATCH\", float(similarity)\n    \n    except Exception as e:\n        print(f\"Error processing images: {str(e)}\")\n        return None, None\n\n# 5. Sử dụng\nif __name__ == \"__main__\":\n    # Khởi tạo FaceNet\n    facenet_graph = load_pb_model(\"/kaggle/input/facenet-model-1/20180402-114759.pb\")\n    facenet_sess = tf.compat.v1.Session(graph=facenet_graph)\n    \n    # Định nghĩa tensor\n    input_tensor = facenet_graph.get_tensor_by_name(\"input:0\")\n    embeddings_tensor = facenet_graph.get_tensor_by_name(\"embeddings:0\")\n    phase_train_tensor = facenet_graph.get_tensor_by_name(\"phase_train:0\")\n    \n    # Test\n    img1 = \"/kaggle/input/vggface2-hq-cropped/VGGface2_HQ_cropped/VGGface2_HQ_cropped/n000002/0001_01.jpg\"\n    img2 = \"/kaggle/input/vggface2-hq-cropped/VGGface2_HQ_cropped/VGGface2_HQ_cropped/n000002/0002_01.jpg\"\n    \n    result, score = compare_faces(img1, img2)\n    if result:\n        print(f\"Result: {result} (Score: {score:.4f})\")\n    \n    # Đóng session\n    facenet_sess.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Clean installation of all required packages\n!pip uninstall -y lz4 python-lz4 mtcnn\n!pip install --no-cache-dir lz4==4.3.2 python-lz4==4.0.2 mtcnn==0.1.1\n!apt-get install -y liblz4-dev\n\nprint(\"\\nPlease RESTART YOUR RUNTIME/KERNEL after this installation!\")\nprint(\"After restarting, run only the main code (skip this installation cell)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport sys\n\n# ========================\n# 1. FACE DETECTION\n# ========================\n\nclass FaceDetector:\n    def __init__(self):\n        self.detector = cv2.CascadeClassifier(\n            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n        )\n    \n    def detect(self, img_path, target_size=(160, 160)):\n        \"\"\"Detect and align face using OpenCV with error handling\"\"\"\n        try:\n            img = cv2.imread(img_path)\n            if img is None:\n                raise ValueError(f\"Could not read image: {img_path}\")\n            \n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            faces = self.detector.detectMultiScale(gray, 1.3, 5)\n            \n            if len(faces) == 0:\n                return None\n                \n            x, y, w, h = faces[0]\n            # Add 25% margin\n            margin = 0.25\n            x = max(0, x - int(w * margin / 2))\n            y = max(0, y - int(h * margin / 2))\n            w = min(img.shape[1] - x, w + int(w * margin))\n            h = min(img.shape[0] - y, h + int(h * margin))\n            \n            face = img[y:y+h, x:x+w]\n            face_rgb = cv2.cvtColor(cv2.resize(face, target_size), cv2.COLOR_BGR2RGB)\n            return face_rgb\n        \n        except Exception as e:\n            print(f\"Face detection warning: {str(e)}\")\n            return None\n\n# ========================\n# 2. FACENET INTEGRATION\n# ========================\n\nclass FaceNetWrapper:\n    def __init__(self, pb_path):\n        self.graph = self._load_graph(pb_path)\n        self.sess = tf.compat.v1.Session(graph=self.graph)\n        \n        # Get tensors with proper error handling\n        try:\n            self.input_tensor = self.graph.get_tensor_by_name(\"input:0\")\n            self.embeddings_tensor = self.graph.get_tensor_by_name(\"embeddings:0\")\n            self.phase_train = self.graph.get_tensor_by_name(\"phase_train:0\")\n        except Exception as e:\n            print(f\"Error loading FaceNet tensors: {str(e)}\")\n            print(\"Available tensors:\")\n            for op in self.graph.get_operations():\n                print(op.name)\n            sys.exit(1)\n    \n    def _load_graph(self, pb_path):\n        \"\"\"Load FaceNet graph from protobuf file\"\"\"\n        with tf.io.gfile.GFile(pb_path, \"rb\") as f:\n            graph_def = tf.compat.v1.GraphDef()\n            graph_def.ParseFromString(f.read())\n        \n        graph = tf.Graph()\n        with graph.as_default():\n            tf.import_graph_def(graph_def, name=\"\")\n        return graph\n    \n    def get_embedding(self, face_img):\n        \"\"\"Get face embedding with proper array handling\"\"\"\n        if face_img is None:\n            return None\n        \n        try:\n            # Convert and normalize image\n            face = (face_img.astype(np.float32) - 127.5) / 128.0\n            if len(face.shape) == 3:\n                face = np.expand_dims(face, axis=0)\n            \n            # Get embedding\n            embedding = self.sess.run(\n                self.embeddings_tensor,\n                feed_dict={self.input_tensor: face, self.phase_train: False}\n            )\n            return embedding.flatten()\n        except Exception as e:\n            print(f\"Embedding extraction error: {str(e)}\")\n            return None\n\n# ========================\n# 3. FACE COMPARISON\n# ========================\n\nclass FaceComparisonSystem:\n    def __init__(self, facenet_path, siamese_path=None):\n        self.face_detector = FaceDetector()\n        self.facenet = FaceNetWrapper(facenet_path)\n        \n        # Load Siamese model if provided\n        self.siamese_model = None\n        if siamese_path:\n            self._load_siamese_model(siamese_path)\n    \n    def _load_siamese_model(self, model_path):\n        \"\"\"Load Siamese model with cosine similarity\"\"\"\n        def cosine_similarity(tensors):\n            x, y = tensors\n            x_norm = tf.norm(x, axis=-1, keepdims=True)\n            y_norm = tf.norm(y, axis=-1, keepdims=True)\n            return tf.reduce_sum(x * y, axis=-1, keepdims=True) / (x_norm * y_norm + 1e-7)\n        \n        try:\n            self.siamese_model = load_model(\n                model_path,\n                custom_objects={'cosine_similarity': cosine_similarity}\n            )\n        except Exception as e:\n            print(f\"Error loading Siamese model: {str(e)}\")\n            self.siamese_model = None\n    \n    def compare(self, img1_path, img2_path, threshold=0.5):\n        \"\"\"Compare two faces with proper array handling\"\"\"\n        try:\n            # Detect faces\n            face1 = self.face_detector.detect(img1_path)\n            face2 = self.face_detector.detect(img2_path)\n            \n            if face1 is None or face2 is None:\n                print(\"Face detection failed for one or both images\")\n                return None, None\n            \n            # Get embeddings\n            emb1 = self.facenet.get_embedding(face1)\n            emb2 = self.facenet.get_embedding(face2)\n            \n            if emb1 is None or emb2 is None:\n                print(\"Embedding extraction failed\")\n                return None, None\n            \n            # Normalize embeddings\n            emb1_norm = (emb1 / np.linalg.norm(emb1)).reshape(1, -1)\n            emb2_norm = (emb2 / np.linalg.norm(emb2)).reshape(1, -1)\n            \n            # Calculate similarity\n            if self.siamese_model:\n                similarity = self.siamese_model.predict([emb1_norm, emb2_norm], verbose=0)[0][0]\n            else:\n                # Fallback to direct cosine similarity\n                similarity = np.dot(emb1_norm, emb2_norm.T)[0][0]\n            \n            return \"MATCH\" if similarity > threshold else \"NO MATCH\", float(similarity)\n        \n        except Exception as e:\n            print(f\"Comparison error: {str(e)}\")\n            return None, None\n\n# ========================\n# 4. MAIN EXECUTION\n# ========================\n\nif __name__ == \"__main__\":\n    # Initialize system\n    system = FaceComparisonSystem(\n        facenet_path=\"/kaggle/input/facenet-model-1/20180402-114759.pb\",\n        siamese_path=\"best_siamese_model.keras\" if os.path.exists(\"best_siamese_model.keras\") else None\n    )\n    \n    # Test images\n    img1 = \"/kaggle/input/dttatt/WIN_20250328_01_04_01_Pro.jpg\"\n    img2 = \"/kaggle/input/trieu1/WIN_20241004_16_01_32_Pro.jpg\"\n    \n    # Verify files exist\n    for img in [img1, img2]:\n        if not os.path.exists(img):\n            print(f\"Error: Image not found - {img}\")\n            sys.exit(1)\n    \n    # Compare faces\n    result, score = system.compare(img1, img2, threshold=0.9)\n    \n    if result:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"RESULT: {result}\")\n        print(f\"SIMILARITY SCORE: {score:.4f}\")\n        print(\"=\"*50)\n    else:\n        print(\"\\nFailed to compare faces. Please check the error messages.\")\n    \n    # Clean up\n    system.facenet.sess.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}