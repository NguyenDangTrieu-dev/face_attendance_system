{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:05:57.187792Z",
     "iopub.status.busy": "2026-01-28T03:05:57.187160Z",
     "iopub.status.idle": "2026-01-28T03:06:22.826734Z",
     "shell.execute_reply": "2026-01-28T03:06:22.825859Z",
     "shell.execute_reply.started": "2026-01-28T03:05:57.187744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q tensorflow keras scikit-learn matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:06:22.829060Z",
     "iopub.status.busy": "2026-01-28T03:06:22.828438Z",
     "iopub.status.idle": "2026-01-28T03:06:22.885999Z",
     "shell.execute_reply": "2026-01-28T03:06:22.885044Z",
     "shell.execute_reply.started": "2026-01-28T03:06:22.829027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load(\"/kaggle/input/facenet-train/vggface2_facenet_train.npz\")\n",
    "val_data   = np.load(\"/kaggle/input/facenet-val/vggface2_facenet_val.npz\")\n",
    "\n",
    "print(\"Train persons:\", len(train_data.files))\n",
    "print(\"Val persons:\", len(val_data.files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:06:22.888299Z",
     "iopub.status.busy": "2026-01-28T03:06:22.887842Z",
     "iopub.status.idle": "2026-01-28T03:07:03.099736Z",
     "shell.execute_reply": "2026-01-28T03:07:03.098733Z",
     "shell.execute_reply.started": "2026-01-28T03:06:22.888220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading embeddings...\n",
      "Train: 480 persons\n",
      "Val: 60 persons\n",
      "üîÑ Creating low-RAM semi-hard triplets (margin=0.4)...\n",
      "   Settings: 10 triplets/person | max 30 neg people | max 40 emb/person\n",
      "‚úÖ Created 4,800 triplets (low-RAM mode)\n",
      "   Shapes ‚Üí Anchor: (4800, 512) | Pos: (4800, 512) | Neg: (4800, 512)\n",
      "üîÑ Creating low-RAM semi-hard triplets (margin=0.4)...\n",
      "   Settings: 6 triplets/person | max 20 neg people | max 30 emb/person\n",
      "‚úÖ Created 360 triplets (low-RAM mode)\n",
      "   Shapes ‚Üí Anchor: (360, 512) | Pos: (360, 512) | Neg: (360, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_triplet_dataset(embeddings_dict, \n",
    "                          num_triplets_per_person=100, \n",
    "                          max_neg_people_sample=100,\n",
    "                          max_neg_emb_per_person=100,\n",
    "                          margin=1.0,\n",
    "                          strategy='semi-hard'):\n",
    "    \"\"\"\n",
    "    T·∫°o triplet v·ªõi semi-hard mining nh∆∞ng gi·ªõi h·∫°n t√†i nguy√™n:\n",
    "    - Ch·ªâ subsample m·ªôt ph·∫ßn nh·ªè negative candidates\n",
    "    - Gi·∫£m s·ªë l∆∞·ª£ng triplet ƒë·ªÉ tr√°nh OOM\n",
    "    \n",
    "    Args:\n",
    "        embeddings_dict: {person_id: [{'embedding': array}, ...]}\n",
    "        num_triplets_per_person: s·ªë triplet m·ªói ng∆∞·ªùi (8-12 l√† an to√†n)\n",
    "        max_neg_people_sample: s·ªë ng∆∞·ªùi kh√°c ƒë∆∞·ª£c xem x√©t (gi·∫£m RAM)\n",
    "        max_neg_emb_per_person: max embedding l·∫•y t·ª´ m·ªói ng∆∞·ªùi kh√°c\n",
    "        margin: d√πng cho semi-hard\n",
    "        strategy: 'semi-hard', 'hard', 'random'\n",
    "    \n",
    "    Returns:\n",
    "        anchors, positives, negatives (numpy arrays)\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Creating low-RAM {strategy} triplets (margin={margin})...\")\n",
    "    print(f\"   Settings: {num_triplets_per_person} triplets/person | \"\n",
    "          f\"max {max_neg_people_sample} neg people | max {max_neg_emb_per_person} emb/person\")\n",
    "    \n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    people = list(embeddings_dict.keys())\n",
    "    \n",
    "    for person_id in people:\n",
    "        person_data = embeddings_dict[person_id]\n",
    "        if len(person_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        person_embs = np.array([d['embedding'] for d in person_data])\n",
    "        \n",
    "        for _ in range(num_triplets_per_person):\n",
    "            idx_a, idx_p = random.sample(range(len(person_embs)), 2)\n",
    "            anchor = person_embs[idx_a]\n",
    "            positive = person_embs[idx_p]\n",
    "            \n",
    "            pos_dist = np.linalg.norm(anchor - positive)\n",
    "            \n",
    "            other_people = [p for p in people if p != person_id]\n",
    "            if len(other_people) > max_neg_people_sample:\n",
    "                other_people = random.sample(other_people, max_neg_people_sample)\n",
    "            \n",
    "            neg_candidates = []\n",
    "            for other_id in other_people:\n",
    "                other_embs = [d['embedding'] for d in embeddings_dict[other_id]]\n",
    "                if len(other_embs) > max_neg_emb_per_person:\n",
    "                    other_embs = random.sample(other_embs, max_neg_emb_per_person)\n",
    "                neg_candidates.extend(other_embs)\n",
    "            \n",
    "            if len(neg_candidates) < 1:\n",
    "                continue \n",
    "            \n",
    "            neg_candidates = np.array(neg_candidates)\n",
    "            neg_dists = np.linalg.norm(neg_candidates - anchor[None, :], axis=1)\n",
    "            if strategy == 'semi-hard':\n",
    "                valid_mask = (neg_dists > pos_dist) & (neg_dists < pos_dist + margin)\n",
    "                if np.any(valid_mask):\n",
    "                    neg_idx = np.random.choice(np.where(valid_mask)[0])\n",
    "                else:\n",
    "                    neg_idx = np.argmin(neg_dists)\n",
    "            elif strategy == 'hard':\n",
    "                neg_idx = np.argmin(neg_dists)\n",
    "            else: \n",
    "                neg_idx = random.randint(0, len(neg_candidates) - 1)\n",
    "            \n",
    "            anchors.append(anchor)\n",
    "            positives.append(positive)\n",
    "            negatives.append(neg_candidates[neg_idx])\n",
    "    \n",
    "    if len(anchors) == 0:\n",
    "        raise ValueError(\"Kh√¥ng t·∫°o ƒë∆∞·ª£c triplet n√†o. Ki·ªÉm tra d·ªØ li·ªáu ho·∫∑c tƒÉng subsample.\")\n",
    "    \n",
    "    anchors    = np.array(anchors,    dtype=np.float32)\n",
    "    positives  = np.array(positives,  dtype=np.float32)\n",
    "    negatives  = np.array(negatives,  dtype=np.float32)\n",
    "    \n",
    "    indices = np.random.permutation(len(anchors))\n",
    "    anchors   = anchors[indices]\n",
    "    positives = positives[indices]\n",
    "    negatives = negatives[indices]\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(anchors):,} triplets (low-RAM mode)\")\n",
    "    print(f\"   Shapes ‚Üí Anchor: {anchors.shape} | Pos: {positives.shape} | Neg: {negatives.shape}\")\n",
    "    \n",
    "    return anchors, positives, negatives\n",
    "\n",
    "\n",
    "def load_and_prepare_triplets():\n",
    "    print(\"üìÇ Loading embeddings...\")\n",
    "\n",
    "    train_npz = np.load('/kaggle/input/facenet-train/vggface2_facenet_train.npz')\n",
    "    val_npz   = np.load('/kaggle/input/facenet-val/vggface2_facenet_val.npz')\n",
    "\n",
    "    train_dict = {\n",
    "        k: [{'embedding': emb} for emb in train_npz[k]]\n",
    "        for k in train_npz.files\n",
    "        if train_npz[k].shape[0] >= 2\n",
    "    }\n",
    "\n",
    "    val_dict = {\n",
    "        k: [{'embedding': emb} for emb in val_npz[k]]\n",
    "        for k in val_npz.files\n",
    "        if val_npz[k].shape[0] >= 2\n",
    "    }\n",
    "\n",
    "    print(f\"Train: {len(train_dict)} persons\")\n",
    "    print(f\"Val: {len(val_dict)} persons\")\n",
    "\n",
    "    train_anchor, train_pos, train_neg = create_triplet_dataset(\n",
    "        train_dict,\n",
    "        num_triplets_per_person=10,\n",
    "        max_neg_people_sample=30,\n",
    "        max_neg_emb_per_person=40,\n",
    "        margin=0.4,\n",
    "        strategy='semi-hard'\n",
    "    )\n",
    "\n",
    "    val_anchor, val_pos, val_neg = create_triplet_dataset(\n",
    "        val_dict,\n",
    "        num_triplets_per_person=6,\n",
    "        max_neg_people_sample=20,\n",
    "        max_neg_emb_per_person=30,\n",
    "        margin=0.4,\n",
    "        strategy='semi-hard'\n",
    "    )\n",
    "\n",
    "    return train_anchor, train_pos, train_neg, val_anchor, val_pos, val_neg\n",
    "train_anchor, train_pos, train_neg, val_anchor, val_pos, val_neg = load_and_prepare_triplets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:03.102423Z",
     "iopub.status.busy": "2026-01-28T03:07:03.101982Z",
     "iopub.status.idle": "2026-01-28T03:07:03.110704Z",
     "shell.execute_reply": "2026-01-28T03:07:03.109575Z",
     "shell.execute_reply.started": "2026-01-28T03:07:03.102394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TripletLoss(keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Triplet Loss with margin\n",
    "    \n",
    "    Loss = max(0, ||f(anchor) - f(positive)||¬≤ - ||f(anchor) - f(negative)||¬≤ + margin)\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_pred contains: [anchor_embedding, positive_embedding, negative_embedding]\n",
    "        \"\"\"\n",
    "        anchor, positive, negative = y_pred[:, 0, :], y_pred[:, 1, :], y_pred[:, 2, :]\n",
    "        \n",
    "        # Compute distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        \n",
    "        basic_loss = pos_dist - neg_dist + self.margin\n",
    "        loss = tf.maximum(basic_loss, 0.0)\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:03.112110Z",
     "iopub.status.busy": "2026-01-28T03:07:03.111852Z",
     "iopub.status.idle": "2026-01-28T03:07:03.275915Z",
     "shell.execute_reply": "2026-01-28T03:07:03.275128Z",
     "shell.execute_reply.started": "2026-01-28T03:07:03.112087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_triplet_network(embedding_dim=512, output_dim=128, margin=0.4):\n",
    "    def create_embedding_network():\n",
    "        \"\"\"M·∫°ng embedding c·∫£i ti·∫øn h∆°n\"\"\"\n",
    "        inputs = layers.Input(shape=(embedding_dim,))\n",
    "        \n",
    "        x = layers.Dense(512, activation='relu', kernel_regularizer='l2')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        x = layers.Dense(output_dim, activation=None)(x)\n",
    "        outputs = layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)\n",
    "        \n",
    "        return keras.Model(inputs, outputs, name='embedding_network')\n",
    "    \n",
    "    embedding_network = create_embedding_network()\n",
    "    \n",
    "    anchor_input = layers.Input(shape=(embedding_dim,), name='anchor')\n",
    "    positive_input = layers.Input(shape=(embedding_dim,), name='positive')\n",
    "    negative_input = layers.Input(shape=(embedding_dim,), name='negative')\n",
    "    \n",
    "    anchor_emb = embedding_network(anchor_input)\n",
    "    positive_emb = embedding_network(positive_input)\n",
    "    negative_emb = embedding_network(negative_input)\n",
    "    \n",
    "    merged = layers.Lambda(\n",
    "        lambda x: tf.stack(x, axis=1),\n",
    "        name='triplet_embeddings'\n",
    "    )([anchor_emb, positive_emb, negative_emb])\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[anchor_input, positive_input, negative_input],\n",
    "        outputs=merged,\n",
    "        name='triplet_network'\n",
    "    )\n",
    "    \n",
    "    return model, embedding_network\n",
    "\n",
    "print(\"\\nüèóÔ∏è Building Triplet Network (margin = 1.0)...\")\n",
    "triplet_model, embedding_network = build_triplet_network(\n",
    "    embedding_dim=512,\n",
    "    output_dim=128,\n",
    "    margin=0.4         \n",
    ")\n",
    "\n",
    "triplet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:03.277806Z",
     "iopub.status.busy": "2026-01-28T03:07:03.277033Z",
     "iopub.status.idle": "2026-01-28T03:07:03.295694Z",
     "shell.execute_reply": "2026-01-28T03:07:03.294650Z",
     "shell.execute_reply.started": "2026-01-28T03:07:03.277778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compile_triplet_model(model, learning_rate=3e-4, margin=1.2):\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5   \n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=TripletLoss(margin=margin)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model compiled | LR={learning_rate} | Margin={margin}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Compile l·∫°i\n",
    "triplet_model = compile_triplet_model(triplet_model, learning_rate=3e-4, margin=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:03.297892Z",
     "iopub.status.busy": "2026-01-28T03:07:03.296908Z",
     "iopub.status.idle": "2026-01-28T03:07:03.308217Z",
     "shell.execute_reply": "2026-01-28T03:07:03.307149Z",
     "shell.execute_reply.started": "2026-01-28T03:07:03.297857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TripletMetricsCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Calculate accuracy metrics during training\"\"\"\n",
    "    \n",
    "    def __init__(self, val_data, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.val_anchor, self.val_pos, self.val_neg = val_data\n",
    "        self.threshold = threshold\n",
    "        self.history = {'val_accuracy': [], 'val_pos_dist': [], 'val_neg_dist': []}\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        embeddings = self.model.predict(\n",
    "            [self.val_anchor, self.val_pos, self.val_neg],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        anchor_emb = embeddings[:, 0, :]\n",
    "        pos_emb = embeddings[:, 1, :]\n",
    "        neg_emb = embeddings[:, 2, :]\n",
    "        \n",
    "        pos_dist = np.sum(np.square(anchor_emb - pos_emb), axis=1)\n",
    "        neg_dist = np.sum(np.square(anchor_emb - neg_emb), axis=1)\n",
    "        \n",
    "        accuracy = np.mean(pos_dist < neg_dist)\n",
    "        \n",
    "        avg_pos_dist = np.mean(pos_dist)\n",
    "        avg_neg_dist = np.mean(neg_dist)\n",
    "        \n",
    "        self.history['val_accuracy'].append(accuracy)\n",
    "        self.history['val_pos_dist'].append(avg_pos_dist)\n",
    "        self.history['val_neg_dist'].append(avg_neg_dist)\n",
    "        \n",
    "        print(f\"\\nüìä Val Accuracy: {accuracy:.4f} | \"\n",
    "              f\"Pos Dist: {avg_pos_dist:.4f} | Neg Dist: {avg_neg_dist:.4f}\")\n",
    "\n",
    "metrics_callback = TripletMetricsCallback(\n",
    "    val_data=(val_anchor, val_pos, val_neg),\n",
    "    threshold=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:03.310202Z",
     "iopub.status.busy": "2026-01-28T03:07:03.309494Z",
     "iopub.status.idle": "2026-01-28T03:07:03.328707Z",
     "shell.execute_reply": "2026-01-28T03:07:03.327722Z",
     "shell.execute_reply.started": "2026-01-28T03:07:03.310163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Callbacks configured!\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    metrics_callback,\n",
    "    \n",
    "    EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ModelCheckpoint(\n",
    "        'best_triplet_model.keras',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:03.330198Z",
     "iopub.status.busy": "2026-01-28T03:07:03.329821Z",
     "iopub.status.idle": "2026-01-28T03:07:58.041798Z",
     "shell.execute_reply": "2026-01-28T03:07:58.040940Z",
     "shell.execute_reply.started": "2026-01-28T03:07:03.330162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING TRAINING WITH TRIPLET LOSS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dummy_train_labels = np.zeros((len(train_anchor), 3, 128))\n",
    "dummy_val_labels = np.zeros((len(val_anchor), 3, 128))\n",
    "\n",
    "history = triplet_model.fit(\n",
    "    [train_anchor, train_pos, train_neg],\n",
    "    dummy_train_labels,\n",
    "    validation_data=([val_anchor, val_pos, val_neg], dummy_val_labels),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:58.044660Z",
     "iopub.status.busy": "2026-01-28T03:07:58.044133Z",
     "iopub.status.idle": "2026-01-28T03:07:59.714272Z",
     "shell.execute_reply": "2026-01-28T03:07:59.713147Z",
     "shell.execute_reply.started": "2026-01-28T03:07:58.044627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_triplet_training(history, metrics_callback):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0].set_title('Triplet Loss Over Epochs')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(metrics_callback.history['val_accuracy'], label='Val Accuracy', color='green')\n",
    "    axes[1].set_title('Validation Accuracy Over Epochs')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Distances\n",
    "    axes[2].plot(metrics_callback.history['val_pos_dist'], label='Positive Distance', color='blue')\n",
    "    axes[2].plot(metrics_callback.history['val_neg_dist'], label='Negative Distance', color='red')\n",
    "    axes[2].set_title('Distances Over Epochs')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('L2 Distance')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('triplet_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Training history plot saved!\")\n",
    "\n",
    "plot_triplet_training(history, metrics_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:07:59.715981Z",
     "iopub.status.busy": "2026-01-28T03:07:59.715501Z",
     "iopub.status.idle": "2026-01-28T03:08:01.102065Z",
     "shell.execute_reply": "2026-01-28T03:08:01.101159Z",
     "shell.execute_reply.started": "2026-01-28T03:07:59.715954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_triplet_model(model, val_anchor, val_pos, val_neg):\n",
    "    \"\"\"Evaluate triplet model\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = model.predict(\n",
    "        [val_anchor, val_pos, val_neg],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    anchor_emb = embeddings[:, 0, :]\n",
    "    pos_emb = embeddings[:, 1, :]\n",
    "    neg_emb = embeddings[:, 2, :]\n",
    "    \n",
    "    pos_dist = np.sum(np.square(anchor_emb - pos_emb), axis=1)\n",
    "    neg_dist = np.sum(np.square(anchor_emb - neg_emb), axis=1)\n",
    "    \n",
    "    print(f\"\\nüìè Distance Statistics:\")\n",
    "    print(f\"   Positive pairs (same person):\")\n",
    "    print(f\"     Mean: {np.mean(pos_dist):.4f} | Std: {np.std(pos_dist):.4f}\")\n",
    "    print(f\"     Min: {np.min(pos_dist):.4f} | Max: {np.max(pos_dist):.4f}\")\n",
    "    print(f\"\\n   Negative pairs (different person):\")\n",
    "    print(f\"     Mean: {np.mean(neg_dist):.4f} | Std: {np.std(neg_dist):.4f}\")\n",
    "    print(f\"     Min: {np.min(neg_dist):.4f} | Max: {np.max(neg_dist):.4f}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = np.mean(pos_dist < neg_dist)\n",
    "    print(f\"\\n‚úÖ Triplet Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   (% of triplets where positive < negative)\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(pos_dist, bins=50, alpha=0.7, label='Positive', color='blue')\n",
    "    plt.hist(neg_dist, bins=50, alpha=0.7, label='Negative', color='red')\n",
    "    plt.xlabel('L2 Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distance Distributions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(pos_dist, neg_dist, alpha=0.5, s=10)\n",
    "    plt.plot([0, max(neg_dist)], [0, max(neg_dist)], 'r--', label='y=x')\n",
    "    plt.xlabel('Positive Distance')\n",
    "    plt.ylabel('Negative Distance')\n",
    "    plt.title('Positive vs Negative Distances')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    all_dists = np.concatenate([pos_dist, neg_dist])\n",
    "    all_labels = np.concatenate([np.ones(len(pos_dist)), np.zeros(len(neg_dist))])\n",
    "    \n",
    "    thresholds = np.linspace(0, max(all_dists), 100)\n",
    "    accuracies = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        pred = (all_dists < thresh).astype(int)\n",
    "        acc = np.mean(pred == all_labels)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    optimal_idx = np.argmax(accuracies)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_accuracy = accuracies[optimal_idx]\n",
    "    \n",
    "    print(f\"\\nüéØ Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"   Accuracy: {optimal_accuracy:.4f}\")\n",
    "    \n",
    "    return optimal_threshold, pos_dist, neg_dist\n",
    "\n",
    "optimal_threshold, pos_dist, neg_dist = evaluate_triplet_model(\n",
    "    triplet_model, val_anchor, val_pos, val_neg\n",
    ")\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_true = np.concatenate([np.ones(len(pos_dist)), np.zeros(len(neg_dist))])\n",
    "\n",
    "similarity_scores = -np.concatenate([pos_dist, neg_dist])\n",
    "\n",
    "fpr, tpr, thresholds_sim = roc_curve(y_true, similarity_scores)\n",
    "\n",
    "dist_thresholds = -thresholds_sim\n",
    "\n",
    "auc_score = auc(fpr, tpr)\n",
    "print(f\"AUC: {auc_score:.4f} (c√†ng g·∫ßn 1 c√†ng t·ªët)\")\n",
    "\n",
    "target_fars = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "print(\"\\nüéØ Threshold theo FAR (tr√™n distance):\")\n",
    "for target in target_fars:\n",
    "    idx = np.argmin(np.abs(fpr - target))\n",
    "    if idx < len(dist_thresholds):\n",
    "        thresh = dist_thresholds[idx]\n",
    "        actual_far = fpr[idx]\n",
    "        frr = 1 - tpr[idx]\n",
    "        print(f\"   Target FAR {target*100:.4f}% ‚Üí Actual FAR {actual_far*100:.4f}% | \"\n",
    "              f\"Threshold distance = {thresh:.4f} | FRR {frr*100:.4f}%\")\n",
    "    else:\n",
    "        print(f\"   Kh√¥ng ƒë·∫°t ƒë∆∞·ª£c FAR {target*100:.4f}% (max FAR ƒë·∫°t {fpr.max()*100:.2f}%)\")\n",
    "\n",
    "if len(dist_thresholds) > 0:\n",
    "    strict_idx = np.argmin(fpr)  \n",
    "    recommended_thresh = dist_thresholds[np.argwhere(fpr <= 0.001)[0][0]] if np.any(fpr <= 0.001) else dist_thresholds[np.argmin(fpr)]\n",
    "    print(f\"\\nRecommended threshold (FAR th·∫•p nh·∫•t): {recommended_thresh:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:08:01.103325Z",
     "iopub.status.busy": "2026-01-28T03:08:01.103029Z",
     "iopub.status.idle": "2026-01-28T03:08:01.139627Z",
     "shell.execute_reply": "2026-01-28T03:08:01.138559Z",
     "shell.execute_reply.started": "2026-01-28T03:08:01.103298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_comparison_model(embedding_network, input_dim=512):\n",
    "    \"\"\"\n",
    "    Build model for comparing two faces\n",
    "    \n",
    "    Input: 2 ArcFace embeddings\n",
    "    Output: Distance score\n",
    "    \"\"\"\n",
    "    \n",
    "    input_1 = layers.Input(shape=(input_dim,), name='embedding_1')\n",
    "    input_2 = layers.Input(shape=(input_dim,), name='embedding_2')\n",
    "    \n",
    "    emb_1 = embedding_network(input_1)\n",
    "    emb_2 = embedding_network(input_2)\n",
    "    \n",
    "    distance = layers.Lambda(\n",
    "        lambda x: tf.sqrt(tf.reduce_sum(tf.square(x[0] - x[1]), axis=1, keepdims=True)),\n",
    "        name='l2_distance'\n",
    "    )([emb_1, emb_2])\n",
    "    \n",
    "    model = Model(inputs=[input_1, input_2], outputs=distance, name='face_comparison')\n",
    "    \n",
    "    return model\n",
    "\n",
    "comparison_model = build_comparison_model(embedding_network, input_dim=512)\n",
    "\n",
    "print(\"\\n‚úÖ Comparison model created!\")\n",
    "comparison_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:08:01.141229Z",
     "iopub.status.busy": "2026-01-28T03:08:01.140780Z",
     "iopub.status.idle": "2026-01-28T03:08:02.100081Z",
     "shell.execute_reply": "2026-01-28T03:08:02.098996Z",
     "shell.execute_reply.started": "2026-01-28T03:08:01.141201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_comparison(model, val_anchor, val_pos, val_neg, threshold, n_samples=10):\n",
    "    \"\"\"Test the comparison model\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîç TESTING COMPARISON MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test positive pairs\n",
    "    print(\"\\n‚úÖ POSITIVE PAIRS (Same Person):\")\n",
    "    for i in range(min(n_samples, len(val_anchor))):\n",
    "        dist = model.predict(\n",
    "            [val_anchor[[i]], val_pos[[i]]],\n",
    "            verbose=0\n",
    "        )[0][0]\n",
    "        \n",
    "        result = \"SAME\" if dist < threshold else \"DIFFERENT\"\n",
    "        status = \"‚úÖ\" if dist < threshold else \"‚ùå\"\n",
    "        \n",
    "        print(f\"  {status} Pair {i+1}: Distance={dist:.4f} ‚Üí {result}\")\n",
    "    \n",
    "    print(\"\\n‚ùå NEGATIVE PAIRS (Different Person):\")\n",
    "    for i in range(min(n_samples, len(val_anchor))):\n",
    "        dist = model.predict(\n",
    "            [val_anchor[[i]], val_neg[[i]]],\n",
    "            verbose=0\n",
    "        )[0][0]\n",
    "        \n",
    "        result = \"SAME\" if dist < threshold else \"DIFFERENT\"\n",
    "        status = \"‚úÖ\" if dist >= threshold else \"‚ùå\"\n",
    "        \n",
    "        print(f\"  {status} Pair {i+1}: Distance={dist:.4f} ‚Üí {result}\")\n",
    "\n",
    "test_comparison(comparison_model, val_anchor, val_pos, val_neg, \n",
    "                optimal_threshold, n_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:08:02.102090Z",
     "iopub.status.busy": "2026-01-28T03:08:02.101325Z",
     "iopub.status.idle": "2026-01-28T03:08:02.183840Z",
     "shell.execute_reply": "2026-01-28T03:08:02.182995Z",
     "shell.execute_reply.started": "2026-01-28T03:08:02.102060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_models_for_production():\n",
    "    \"\"\"Save models for production use\"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Saving models...\")\n",
    "    \n",
    "    embedding_network.save('embedding_network.keras')\n",
    "    print(\"‚úÖ Saved: embedding_network.keras\")\n",
    "    \n",
    "    comparison_model.save('face_comparison_model.keras')\n",
    "    print(\"‚úÖ Saved: face_comparison_model.keras\")\n",
    "    \n",
    "    np.save('optimal_threshold.npy', optimal_threshold)\n",
    "    print(f\"‚úÖ Saved: optimal_threshold.npy (value: {optimal_threshold:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üì¶ MODELS SAVED SUCCESSFULLY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "save_models_for_production()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T03:08:02.185409Z",
     "iopub.status.busy": "2026-01-28T03:08:02.184986Z",
     "iopub.status.idle": "2026-01-28T03:08:02.405056Z",
     "shell.execute_reply": "2026-01-28T03:08:02.404340Z",
     "shell.execute_reply.started": "2026-01-28T03:08:02.185382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üîç Evaluating Triplet Model using EMBEDDINGS (no images)\")\n",
    "\n",
    "X1 = np.vstack([val_anchor, val_anchor])\n",
    "X2 = np.vstack([val_pos, val_neg])\n",
    "\n",
    "y_true = np.array(\n",
    "    [1] * len(val_anchor) +   # anchor‚Äìpositive\n",
    "    [0] * len(val_anchor)     # anchor‚Äìnegative\n",
    ")\n",
    "\n",
    "distances = np.linalg.norm(X1 - X2, axis=1)\n",
    "\n",
    "thresholds = np.linspace(distances.min(), distances.max(), 300)\n",
    "\n",
    "best_acc = 0\n",
    "best_th = 0\n",
    "\n",
    "for th in thresholds:\n",
    "    y_pred = (distances < th).astype(int)\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_th = th\n",
    "y_pred_final = (distances < best_th).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Best Threshold: {best_th:.4f}\")\n",
    "print(f\"‚úÖ Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred_final,\n",
    "    target_names=[\"Kh√¥ng kh·ªõp\", \"Kh·ªõp\"]\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_final)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9330306,
     "sourceId": 14606964,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9330311,
     "sourceId": 14606970,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9352575,
     "sourceId": 14640654,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9352595,
     "sourceId": 14640715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
